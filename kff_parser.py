from bs4 import BeautifulSoup
from subprocess import call
import pandas as pd
import os
import glob
import json

##### PURPOSE OF THIS SCRIPT: From a wayback-machine-downloader directory of the KFF data page,
# write a list of csvs of the tables of interest, named by day. After using this script, make each type of file a folder,
# and run import-kff.R in RStudio to combine into a panel.


# Call this terminal command, if you need to (ensure the target directory is empty first) :
#call('wayback_machine_downloader https://www.kff.org/health-costs/issue-brief/state-data-and-policy-actions-to-address-coronavirus/ -es -d ~/covid/kff-snapshots')

# First search through the list of folders generated by the wayback-machine-downloader command
# and retrieve the paths of the html files, creating a list of paths to visit
kff_snapshots_paths = []
kff_snapshots = {}
dirList = glob.glob("/Users/georgetyler/covid/kff-snapshots/*/health-costs/issue-brief/state-data-and-policy-actions-to-address-coronavirus/*.html")
for d in dirList:
 kff_snapshots_paths.append(d)

def extract_tables(soup):
    '''Extract the tables of interest from the raw .html file. Returns a list of tables,
     which are themselves formatted as a list of lists.'''
    divs = soup.findAll('div')
    tables = []

    for div in divs:
        if 'data-app-js' in div.attrs.keys():
            jsobject = div['data-app-js']
            d = json.loads(jsobject) # We use json to parse the list of lists
            if d['gdocs_object']: # Do this to avoid picking up empty gdocs objects
                tables.append(d['gdocs_object'])
    return tables

def clean_df(df):
    '''Clean the dataset of its shared weirdness.'''
    # Remove US and a random row
    df = df.drop(df.index[[1, 2]])

    # Make the first row the headers
    df.columns = df.iloc[0]
    df = df[1:]
    return df

def add_date(df, date):
    datecol = [date] * len(df.index)
    df['date'] = datecol
    return df

# Now, we loop over the html files and call extract_tables to get our tables of interest. We the list of tables in a dict
# with the key as the timestamp of the html file
for i in kff_snapshots_paths:
    HtmlFile = open(i, 'r', encoding='utf-8')
    soup = BeautifulSoup(HtmlFile, 'html.parser')
    i = i.lstrip('/Users/georgetyler/covid/kff-snapshots/')
    i = i.rstrip('/health-costs/issue-brief/state-data-and-policy-actions-to-address-coronavirus/index.html')
    kff_snapshots[i] = extract_tables(soup)
    HtmlFile.close()

table_names = ['cases-deaths', 'social-distancing-policies', 'health-policies', 'covid-tests', 'number-at-risk', 'insurance-coverage', 'insurance-deductible', 'hospital-capacity', 'flu-pneumonia']

# Now, we loop through the files and in turn through the list of tables in the file. We create a pandas dataframe from
# each table, clean it up, and export the two we're interested in as a csv.
for k in kff_snapshots:
    counter = 1
    for i in kff_snapshots[k]:
        df = pd.DataFrame(i[0][1])
        date = str(k)[:-6]
        date = date[:4] + '-' + date[4:6] + '-' + date[-2:]
        df = clean_df(df)
        df = add_date(df, date)
        # Un-comment this if you want to export every table found, numbered
        #df.to_csv(date + '_' + str(counter) + '.csv' )
        #counter += 1
        if 'Number of COVID-19 Cases' in df:
            continue
        elif 'Non-Essential Business Closures' in df:
            table_name = table_names[1]
            df.to_csv(date + '_' + table_name + '.csv')
        elif 'Waive Cost Sharing for COVID-19 Treatment' in df:
            table_name = table_names[2]
            df.to_csv(date + '_' + table_name + '.csv')
        else:
            continue